---
title: "[Review] DeepFool: a simple and accurate method to fool deep neural networks"
category: [paper]
tag: [attack]
comments: true
toc: true
toc_label: 목차
toc_sticky: true
---
# Abtract
인공지능 모델이 적대적 공격에 취약하다는 사실이 잘 알려져 있다.
따라서 모델은 공격에 대한 강건성을 확보해야 하는데, 강건성을 평가하기 위한 효율적이고 정량적인 방법이 없다.
따라서 본 논문은 모델의 강건성을 평가하기 위해, 비용을 최소한으로 들이는 적대적 이미지 생성 방법을 제안한다.

# Intro
적대적 공격은 이미지에 적대적 perturbation을 추가하는 과정이기 때문에, perturbation을 결정하는 과정이 중요하다. 입력 이미지 $x$에 대해 가해지는 perturbation $\Delta(x;\hat{k})$는 다음과 같이 정의할 수 있다.
$$
\begin{align}
\Delta(x;\hat{k}) := min_r||r||_2 \:subject\: to\: \hat{k}(x+r)\neq \hat{k}(x)
\end{align}
$$
$\Delta(x;\hat{k})$는 $\hat{k}$라는 classifier에 적대적 공격을 수행했을 때, $x$에 가해지는 perturbation을 의미한다. 만약 적대적 공격이 성공하기 위해 필요한 perturbation이 작다면
해당 모델은 적은 변조로 눈에 띄지 않게 적대적 이미지를 생성하기 때문에 강건성이 낮다고 평가할 수 있다. 반대로 perturbation이 크다면 모델을 속이기 위해 많은 변조가 필요하기 때문에 강건성이 높다고 평가할 수 있다.

하지만 수식 1은 분류기 k에 대해 특정 이미지 $x$에 대한 강건성을 의미하기 때문에, 이를 분류기 $k$에 대한 강건성으로 확장할 필요가 있다.
분류기 $k$에 일반화된 강건성은 다음 수식으로 표현할 수 있다.
$$
\begin{align}
\rho_{adv}(\hat{k})=\mathbb{E}_x[\frac{\Delta(x;\hat{k})}{||x||_2}]
\end{align}
$$
수식 2의 의미는 분류기 $k$에 대한 강건성을 모든 입력 $x$의 적대적 perturbation의 평균으로 근사한다는 것이다.

그러나 수식 2를 모델의 강건성 나타내는 정량적 지표로 사용하기 위해서는 perturbation $r$이 $\hat{k}(x+r) \neq \hat{k}(x)$를 만족하는 최솟값이라는 보장이 있어야 한다.
따라서 본 논문은 모델을 속이는 최소한의 perturbation을 찾는 방법을 제안한다.

# DeepFool for binary classifier
모델을 속이는 최소한의 perturbation을 찾는 문제를 풀기 위해, 우선 분류 모델 $\hat{k}$를 이진 분류기 $sign(f(x))$로 설정한다. $f(\cdot)$은 입력 $x$가 들어오면 스칼라 값을 출력한다.
다시 말해서 $f:R^n \rightarrow R$이며 $f(x)$가 0이 되면 공격이 성공한 것으로 본다. 만약 $f(x)=w^Tx+b$라고 하면 $f(x_0)$는 그림 1과 같다.
<figure>
  <img src="https://github.com/user-attachments/assets/1c8fc6f9-78ad-49a7-b35d-6247166cc585" alt="linear binary classifier">
  <figcaption>그림 1. f(x_0) 도식화.</figcaption>
</figure>

$f(x_0)$에 최소한의 perturbation $\Delta(x_0;,f)$를 추가해 공격이 성공하기 위해서는 $x_0$에서 $f(x)$에 수직으로 사영하면 된다. 따라서 $x_0$에서 시작하여 $f(x)$에 도달하는 수직 벡터 $r_*(x)$가 최소한의 perturbation이 되며 아래와 같이 표현되며
$$
\begin{align}
r_*(x_0):&=arg min||r||_2 \: subject \: to \: sign(f(x_0+r))\neq sign(f(x_0)) \\
&=-\frac{f(x_0)}{||w||^2_2}w
\end{align}
$$

다음과 같이 유도된다.
$$
\begin{align}
f(x_0)&=wx_0+b \\
f(x_0+r)&=w(x_0+r)+b=0 \\
&=wx_0+wr+b \\
&=f(x_0)+wr=0 \\
f(x_0)&=-wr\\
r_*(x)&=\alpha w (∵ r이 w의 상수배)\\
f(x_0)&=-\alpha w^2\\
\alpha &= -\frac{f(x_0)}{w^2} = -\frac{f(x_0)}{||w||^2_2} \\
r_*(x)&=-\frac{f(x_0)}{||w||^2_2}w
\end{align}
$$
