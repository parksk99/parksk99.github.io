---
title: "[Review] DeepSeek"
category: [paper]
tag: [network]
comments: true
toc: true
toc_label: 목차
toc_sticky: true
---
# DeepSeek
DeepSeek는 DeepSeek에서 만든 언어 모델로, 적은 비용으로 ChatGPT와 비슷한 성능을 낸다고 하는 모델이다.
여러 버전이 있는데 V1 -> V2 -> V3 -> R1 순으로 발전되었다. 각 버전에 구조적인 변화나 학습 방법을 변경하여 성능을 끌어올리는데, 그 특징은 다음 그림과 같다.   
<div align="center">
  <img src="https://github.com/user-attachments/assets/76e01434-6554-4224-9385-c0d4301dc293" width="250" height="200">
</div>

V1과 V2에서 Multi-Head Latent Attention(MLA)와 Mixture of Experts(MoE) 구조를 채택하고, V3에서 Multi-Token Predict(MTP)와 Auxiliary-Loss free load balancing 기법을 적용한다.
마지막으로 위와 같은 구조적이고 메커니즘적인 변화로 학습한 V3 모델에 강화학습을 적용하여 추론 능력을 극대화함으로써 Chat-GPT의 o1과 맞먹는 성능을 가진 R1 모델을 만든다.

중국이 미국의 반도체 규제로 인해 적은 메모리와 bandwidth를 가진 GPU로 성능을 극대화하기 위해 여러 적용한 여러 기술을 크게 다음의 다섯 가지 기술로 나누었고 bottom-up 방식으로 하나씩 정리했다.

- [ ] Multi-Head Latent Attention(MLA)
- [X] Mixture of Experts(MoE)
- [ ] Multi-Token Predict(MTP)
- [ ] Auxiliary-loss free load balancing
- [ ] Reinforcement learning

# Mixture of Experts(MoE)
## Q?
Inference 할 때 마다 사용하지 않을 파라미터를 끄고 사용할 파라미터를 키는건가? 여기에 발생하는 오버헤드는?

## Introduction to MoE
모델의 파라미터를 키우면 모델의 capability가 향상된다는 것은 자명하다. 하지만 자원은 한정되어 있기 때문에 scaling에도 한계가 있다.
Scaling의 이점은 취하면서 주어진 자원에 타협하기 위해 MoE가 제안되었다.
MoE는 모델의 파라미터 중 전문가들(experts)을 양성해, 입력이 들어왔을 때 필요한 expert만 활성화하는 구조를 말한다.
이 구조를 따름으로써 모델의 전체 파라미터가 활성화되는 것이 아니기 떄문에 모든 파라미터를 사용했을 때 보다 적은 메모리를 요구해 메모리를 절약할 수 있으며 절약한 메모리로 모델의 전체 크기를 키워 capability를 높인다.
보다 적은 메모리로 experts의 조합을 통해 문제를 해결하는 것이 MoE의 특징이다.

FFN의 입력을 $u_t$라고 할 때 MoE의 출력 $h_t$수식으로 나타내면 다음과 같다.
$$
\begin{aligned}
h_t &=\Sigma_{i=1}^N(g_{i,t}FFN_i(u_t)) + u_t   \\
g_{i,t} &=\left\{
    \begin{array}{ll}
        s_{i,t}, & s_{i,t} \in Topk(\{s_{j,t}|1 \leq j<N\},K) \\
        0, & \text{otherwise},
    \end{array}
\right.
\\ s_{i,t}&=Softmax_i(u_t^T e_i),
\end{aligned}
$$
N은 FFN의 개수, t는 t번째 토큰을 의미한다. 입력 $u_t$에 i번째 FFN이 활성화될지 결정하는 벡터 $e_i$를 곱해 $i$를 파라미터로 하는 softmax를 거친다.
그러면 t번째 토큰이 i번째 FFN에 대해 활성화되는 정도를 $s_{i,t}$로 나타낼 수 있다.
이후, 상위 K개의 $s_{i,t}$만 취해 모두 더한다. 마지막으로 residual connection을 위해 원본 $u_t$를 더하면 토큰 하나에 대한 MoE Layer의 출력을 구할 수 있다.

## Router
MoE의 첫번째 관건은 입력이 들어왔을 때 어떤 experts를 활성화할지 결정하는 것이다.
활성화할 experts를 선택하는 방법은 router이다. Router는 아래 그림과 같다.   
<div align="center">
  <img src="https://github.com/user-attachments/assets/0fb4def6-d12e-42b8-8905-7fcee803c53b">
</div>
먼저 왼쪽의 (a)가 이전 수식에서 설명한 가장 기본적인 MoE이다. Router의 K값이 2라는 것은, 활성화할 N개의 FFN 중 상위 두 개의 값을 선택하는 것이다.
다음으로 가운데의 (b)는 fine-grained expert segmentation으로, 각 FFN을 m개의 experts로 세분화하는 것이다. 그림을 보면 K는 4이고, FFN의 개수가 2N이 되었기 때문에 m은 2라고 볼 수 있다.
FFN을 세분화하면 동일한 비용으로 훨씬 더 많은 경우의 수를 표현할 수 있다. 예를 들어 N이 16이고 k가 2라고 했을 때, (a)의 경우에 $_16C_2$, 즉 120가지 경우의 수가 있다.
하지만 (b)의 경우 m이 4가 되어 하나의 FFN을 네 개로 쪼갠다면, 동일한 메모리로 $_64C_8$, 즉 4,426,165,368개의 경우의 수를 표현할 수 있다.

마지막으로 오른쪽의 (c)는 DeepSeek에 적용된 MoE 구조로 (b)의 구조에 shared expert를 추가한 것이다.
다른 입력이 공통의 지식을 필요로 할 경우가 생기는데, 만약 대부분의 도메인에서 필요한 기본적인 지식이 있다면 학습을 진행함에 따라 여러 experts가 동일한 지식을 공유하게 될 수 있다.
이것은 지식의 중복이기 때문에 expert가 제기능을 한다고 보기 어렵다. 따라서 DeepSeek에서는 shared expert를 두어 이 문제를 해결했다.
여러 도메인 간에 공유되는 지식은 shared expert가 학습하도록 해서 지식의 중복을 제거한다.

## Load Balancer
MoE의 두 번째 관건은 expert를 얼만큼, 어느 규칙에 따라 활성화할지 결정하는 것이다.
만약 K의 값을 키워 FFN의 개수인 N에 도달한다면, 모델의 모든 파라미터를 활성화하는 것이기 때문에 MoE를 적용하는 의미가 없어진다.
반대로 K의 값을 너무 줄인다면, 좋은 성능을 낼 수 없다.
또한 expert를 활성화 할 때, 특정 expert만 과도하게 자주 활성화된다면 다른 experts는 제 기능을 하지 못하게 되고, 반대로 특정 expert가 거의 사용되지 않는다면 그것은 없는 expert와 마찬가지가 된다.

게다가 거대 언어모델의 환경을 고려하면, 특정 GPU에 있는 네트워크만 과도하게 활성화되는 것도 문제가 된다.
특정 GPU의 네트워크만 활성화된다면, 부하가 그 GPU에만 걸리게 되어 병목현상이 발생한다.

따라서, experts 활성화를 균형있게 조절하는 것이 중요하다. 하지만 여기서 사용하는 load balancer는 DeepSeek가 발전함에 따라 사용하지 않기 때문에 일단 다루지 않는다.
# References
1. DeepSeek LLM: Scaling Open-Source Language Models with Longtermism. arXiv, 2024년 1월 5일.
2. DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models. arXiv, 2024년 1월 11일.
3. DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model. arXiv, 2024년 6월 19일.
4. Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts
5. DeepSeek-V3 Technical Report. arXiv, 2024년 12월 27일
6. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv, 2025년 1월 22일. 